import torch
from gpytorch import models, priors, constraints
from gpytorch import means, kernels, distributions, likelihoods, mlls, settings
from pyro.infer.mcmc import NUTS, MCMC
import numpy as np
import pods

from visualization import plot_gp


class ExactGPModel(models.ExactGP):

    def __init__(self, x_train, y_train, likelihood):
        super().__init__(x_train, y_train, likelihood)
        self.mean_module = means.ConstantMean(prior=priors.UniformPrior(0, 5))
        # periodic_kernel = kernels.PeriodicKernel(lengthscale_prior=priors.UniformPrior(0.01, 0.5),
        #                                          period_length_prior=priors.UniformPrior(0.05, 2.5))

        rbf_kernel = kernels.RBFKernel(lengthscale_prior=priors.UniformPrior(1, 30))
        self.covar_module = kernels.ScaleKernel(rbf_kernel, outputscale_prior=priors.UniformPrior(0, 1))

    def forward(self, x):
        mean_x = self.mean_module(x)
        covar_x = self.covar_module(x)
        return distributions.MultivariateNormal(mean_x, covar_x)


num_samples = 100
warmup_steps = 200

# x_train = torch.linspace(0, 1, 15)
# y_train = torch.sin(x_train * (2 * np.pi))

data = pods.datasets.olympic_marathon_men()
x_train = torch.from_numpy(data["X"]).squeeze(-1)
y_train = torch.from_numpy(data["Y"]).squeeze(-1)


likelihood = likelihoods.GaussianLikelihood(noise_constraint=constraints.Positive(),
                                            noise_prior=priors.UniformPrior(0.01, 0.3))

model = ExactGPModel(x_train, y_train, likelihood)

# model.mean_module.register_prior("mean_prior", priors.UniformPrior(-1, 1), "constant")
# model.covar_module.base_kernel.register_prior("lengthscale_prior", priors.UniformPrior(0.01, 0.5), "lengthscale")
# model.covar_module.base_kernel.register_prior("period_length_prior", priors.UniformPrior(0.05, 2.5), "period_length")
# model.covar_module.register_prior("outputscale_prior", priors.UniformPrior(1, 2), "outputscale")
# likelihood.register_prior("noise_prior", priors.UniformPrior(0.05, 0.3), "noise")

marginal_loglikelihood = mlls.ExactMarginalLogLikelihood(likelihood, model)


def pyro_model(x, y):
    model.pyro_sample_from_prior()
    output = model(x)
    loss = marginal_loglikelihood.pyro_factor(output, y)
    return y

nuts_kernel = NUTS(pyro_model, adapt_step_size=True)
mcmc_run = MCMC(nuts_kernel, num_samples=num_samples, warmup_steps=warmup_steps)
mcmc_run.run(x_train, y_train)

#we load the samples generated by NUTS in to the model.
# This converts model from a single GP to a batch of num_samples GPs, in this case 100.
model.pyro_load_from_samples(mcmc_run.get_samples())
model.eval()
x_test = torch.from_numpy(np.linspace(1870, 2030, 200)[:, np.newaxis])
expanded_test_x = x_test.unsqueeze(0).repeat(num_samples, 1, 1)
output = model(expanded_test_x)


import matplotlib.pyplot as plt

with torch.no_grad():
    # Initialize plot
    f, ax = plt.subplots(1, 1, figsize=(16, 9))

    # Plot training data as black stars
    ax.plot(x_train.numpy(), y_train.numpy(), 'k*', zorder=10)

    for i in range(num_samples):
        # Plot predictive means as blue line
        ax.plot(x_test.numpy(), output.mean[i].detach().numpy(), 'b', linewidth=0.3)

    # Shade between the lower and upper confidence bounds
    # ax.fill_between(test_x.numpy(), lower.numpy(), upper.numpy(), alpha=0.5)
    ax.set_ylim([0, 7])
    ax.legend(['Observed Data', 'Sampled Means'])

    plt.show()